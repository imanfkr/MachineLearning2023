# -*- coding: utf-8 -*-
"""Question(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WMjzOtgFxqOYmndKIb_JvGrOhluYvQQO

### Download Data
"""

!pip install --upgrade --no-cache-dir gdown
!gdown 1iPrPZZKjKAwEumQ3s8kGCvIMURt1TaF-

"""### Read .csv & Call .info"""

import pandas as pd
data = pd.read_csv("/content/Perceptron.csv")
data
# The first and second columns of the CSV file related to this data set are related to the features and the third column is related to the class of each data.

"""### Imports"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Perceptron
import matplotlib.pyplot as plt
from mlxtend.plotting import plot_decision_regions

x = data.iloc[:, :-1].values # x is features
y = data.iloc[:, -1].values  # y is target
# Transforming y values from {-1, 1} to {0, 1}
y = np.where(y == -1, 0, 1)
y

"""### Train-Test Split"""

# PART1
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
x_train.shape, x_test.shape, y_train.shape, y_test.shape

"""### Perceptron"""

# Initialize Perceptron classifier with a different threshold (example: 0.5)
model = Perceptron()

# Train the perceptron with the new threshold
model.fit(x_train, y_train)

"""## Accuracy"""

# PART2
# Accuracy on train and test data with the new threshold
train_accuracy = model.score(x_train, y_train)
test_accuracy = model.score(x_test, y_test)

print(f"Accuracy on train set with new threshold: {train_accuracy}")
print(f"Accuracy on test set with new threshold: {test_accuracy}")
# Convert y_test to integer type
y_test = y_test.astype(np.int)

"""## Plot"""

x1_min, x2_min = x_test.min(0)
x1_max, x2_max = x_test.max(0)
n=400
x1r = np.linspace(x1_min, x1_max, n)
x2r = np.linspace(x2_min, x2_max, n)
x1m, x2m = np.meshgrid(x1r, x2r)
xm = np.stack((x1m.flatten(), x2m.flatten()),axis=1)
ym = model.decision_function(xm)
colors = np.array(['blue', 'red'])
plt.scatter(x_test[:, 0], x_test[:, 1], c=colors[y_test])
plt.contour(x1m, x2m, ym.reshape(x1m.shape), levels=[0,1])
plt.show()

"""# **Neuron (from Scratch)**"""

def relu(x):
    return np.maximum(0, x)
def sigmoid(x):
    return 1/(1+np.exp(-x))
def tanh(x):
    return np.tanh(x)
def bce(y, y_hat):
    return np.mean(-(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))
def mse(y, y_hat):
    return np.mean((y - y_hat)**2)
def accuracy(y, y_hat, t=0.5):
    y_hat = np.where(y_hat<t, 0, 1)
    acc = np.sum(y == y_hat) / len(y)
    return acc
class Neuron:

    def __init__(self, in_features, threshold, af=None, loss_fn=mse, n_iter=100, eta=0.1, verbose=True):
        self.in_features = in_features
        # weight & bias
        self.w = np.random.randn(in_features, 1)
        self.threshold = threshold
        self.af = af
        self.loss_fn = loss_fn
        self.loss_hist = []
        self.w_grad = None
        self.n_iter = n_iter
        self.eta = eta
        self.verbose = verbose

    def predict(self, x):
        # x: [n_samples, in_features]
        y_hat = x @ self.w + self.threshold
        y_hat = y_hat if self.af is None else self.af(y_hat)
        return y_hat

    def decision_function(self, x):
        # x: [n_samples, in_features]
        y_hat = x @ self.w + self.threshold
        return y_hat

    def fit(self, x, y):
        for i in range(self.n_iter):
            y_hat = self.predict(x)
            loss = self.loss_fn(y, y_hat)
            self.loss_hist.append(loss)
            self.gradient(x, y, y_hat)
            self.gradient_descent()
            if self.verbose & (i % 10 == 0):
                print(f'Iter={i}, Loss={loss:.4}')

    def gradient(self, x, y, y_hat):
        self.w_grad = (x.T @ (y_hat - y)) / len(y)

    def gradient_descent(self):
        self.w -= self.eta * self.threshold

    def __repr__(self):
        af_name = self.af.__name__ if self.af is not None else None
        loss_fn_name = self.loss_fn.__name__ if self.loss_fn is not None else None
        return f'Neuron({self.in_features}, {self.threshold}, {af_name}, {loss_fn_name}, {self.n_iter}, {self.eta}, {self.verbose})'

    def parameters(self):
        return {'w': self.w, 'threshold': self.threshold}

"""# Plot"""

# Create a scatter plot
plt.figure(figsize=(8, 6))  # Set the figure size
scatter_class_0 = plt.scatter(x[y == 0, 0], x[y == 0, 1], color='blue', label='Class -1', edgecolors='k', marker='o')
scatter_class_1 = plt.scatter(x[y == 1, 0], x[y == 1, 1], color='orange', label='Class 1', edgecolors='k', marker='o')
plt.xlabel("1'st feature")
plt.ylabel("2'nd feature")
plt.title('Scatter Plot of Dataset')  # Title for the plot
plt.legend(handles=[scatter_class_0, scatter_class_1], title='Classes',loc="upper left")
plt.grid(alpha=0.2)  # Display grid lines

# Splitting the dataset into the Training set and Test set (80/20 split)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=93, stratify=y, shuffle=True)

# Display the dimensions of the training and testing sets
print(f'Dimensions of the training features: {x_train.shape}')
print(f'Dimensions of the training target: {y_train.shape}')
print(f'Dimensions of the testing features: {x_test.shape}')
print(f'Dimensions of the testing target: {y_test.shape}')

neuron = Neuron(in_features=2, threshold=0.1, af=sigmoid, loss_fn=bce, n_iter=500, eta=0.1, verbose=True)
neuron.fit(x_train, y_train[:, None])
print(f'Neuron specification: {neuron}')
print(f'Neuron parameters: {neuron.parameters()}')

"""# Loss Func"""

plt.figure(figsize=(8, 6))
plt.plot(neuron.loss_hist)
plt.xlabel("Number of Epochs")
plt.ylabel("Error")
plt.title('Loss Function')
plt.grid(alpha=0.5)

"""#accuracy"""

y_hat = neuron.predict(x_test)
accuracy(y_test[:, None], y_hat, t=0.5)

"""# Showing the decision boundary and safe area of two classes"""

# Define the range of values for x1 and x2
x1_min, x2_min = x_test.min(0) - 0.5
x1_max, x2_max = x_test.max(0) + 0.5

# Generate a meshgrid of points
n = 500
x1r = np.linspace(x1_min, x1_max, n)
x2r = np.linspace(x2_min, x2_max, n)
x1m, x2m = np.meshgrid(x1r, x2r)

# Flatten the meshgrid points and predict the class labels
xm = np.stack((x1m.flatten(), x2m.flatten()), axis=1)
ym = neuron.decision_function(xm)  # Use decision_function instead of predict

# Plot the decision region
plt.contourf(x1m, x2m, ym.reshape(x1m.shape), levels=[-0.5, 0.5])

# Scatter plot for the test data with different markers
colors = np.array(['blue', 'red'])
plt.scatter(x_test[:, 0], x_test[:, 1], c=colors[y_test], edgecolors='k', marker='o', s=80, linewidth=1, label='Test Data')

# Set labels and legend
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Decision Region of Perceptron')
plt.legend()
plt.grid(alpha=0.2)  # Display grid lines

# Add legend for class -1 and class 1
legend_elements = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Class -1'),
                   plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Class 1')]
plt.legend(handles=legend_elements, loc='upper left')

# Show the plot
plt.show()

"""# changing bias"""

neuron = Neuron(in_features=2, threshold=-0.1, af=sigmoid, loss_fn=bce, n_iter=500, eta=0.1, verbose=True)
neuron.fit(x_train, y_train[:, None])
print(f'Neuron specification: {neuron}')
print(f'Neuron parameters: {neuron.parameters()}')

plt.figure(figsize=(8, 6))
plt.plot(neuron.loss_hist)
plt.xlabel("Number of Epochs")
plt.ylabel("Error")
plt.title('Loss Function')
plt.grid(alpha=0.5)

"""# accuracy"""

y_hat = neuron.predict(x_test)
accuracy(y_test[:, None], y_hat, t=0.5)

# Define the range of values for x1 and x2
x1_min, x2_min = x_test.min(0) - 0.5
x1_max, x2_max = x_test.max(0) + 0.5

# Generate a meshgrid of points
n = 500
x1r = np.linspace(x1_min, x1_max, n)
x2r = np.linspace(x2_min, x2_max, n)
x1m, x2m = np.meshgrid(x1r, x2r)

# Flatten the meshgrid points and predict the class labels
xm = np.stack((x1m.flatten(), x2m.flatten()), axis=1)
ym = neuron.decision_function(xm)  # Use decision_function instead of predict

# Plot the decision region
plt.contourf(x1m, x2m, ym.reshape(x1m.shape), levels=[-0.5, 0.5])

# Scatter plot for the test data with different markers
colors = np.array(['blue', 'red'])
plt.scatter(x_test[:, 0], x_test[:, 1], c=colors[y_test], edgecolors='k', marker='o', s=80, linewidth=1, label='Test Data')

# Set labels and legend
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Decision Region of Perceptron')
plt.legend()
plt.grid(alpha=0.2)  # Display grid lines

# Add legend for class -1 and class 1
legend_elements = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Class -1'),
                   plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Class 1')]
plt.legend(handles=legend_elements, loc='upper left')

# Show the plot
plt.show()

"""## bias elemination"""

neuron = Neuron(in_features=2, threshold=0, af=sigmoid, loss_fn=bce, n_iter=500, eta=0.1, verbose=True)
neuron.fit(x_train, y_train[:, None])
print(f'Neuron specification: {neuron}')
print(f'Neuron parameters: {neuron.parameters()}')

plt.figure(figsize=(8, 6))
plt.plot(neuron.loss_hist)
plt.xlabel("Number of Epochs")
plt.ylabel("Error")
plt.title('Loss Function')
plt.grid(alpha=0.5)

"""# accuracy"""

y_hat = neuron.predict(x_test)
accuracy(y_test[:, None], y_hat, t=0.5)

# Define the range of values for x1 and x2
x1_min, x2_min = x_test.min(0) - 0.5
x1_max, x2_max = x_test.max(0) + 0.5

# Generate a meshgrid of points
n = 500
x1r = np.linspace(x1_min, x1_max, n)
x2r = np.linspace(x2_min, x2_max, n)
x1m, x2m = np.meshgrid(x1r, x2r)

# Flatten the meshgrid points and predict the class labels
xm = np.stack((x1m.flatten(), x2m.flatten()), axis=1)
ym = neuron.decision_function(xm)  # Use decision_function instead of predict

# Plot the decision region
plt.contourf(x1m, x2m, ym.reshape(x1m.shape), levels=[-0.5, 0.5])

# Scatter plot for the test data with different markers
colors = np.array(['blue', 'red'])
plt.scatter(x_test[:, 0], x_test[:, 1], c=colors[y_test], edgecolors='k', marker='o', s=80, linewidth=1, label='Test Data')

# Set labels and legend
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Decision Region of Perceptron')
plt.legend()
plt.grid(alpha=0.2)  # Display grid lines

# Add legend for class -1 and class 1
legend_elements = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Class -1'),
                   plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Class 1')]
plt.legend(handles=legend_elements, loc='upper left')

# Show the plot
plt.show()
# -*- coding: utf-8 -*-
"""Question(3)_Project(1)_Part(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u6dMXZwg5CGdjRn4CjiYTvb8YsFTA9S7
"""

# part 1 of question 3 :


#! pip install --upgrade --no-cache-dir gdown
! gdown 1gQQfj0cY5pQRngCQulY5ylD0FZa9LIZI

# part 2 of question 3 :

import pandas as pd
import numpy as np
df = pd.read_csv("/content/heart_disease_health_indicators.csv")
df

# Separate the data into two classes
class_0_data = df[df['HeartDiseaseorAttack'] == 0].head(100)
class_1_data = df[df['HeartDiseaseorAttack'] == 1].head(100)

# Create two new DataFrames for each class
df_class_0 = pd.DataFrame(class_0_data, copy=True)
df_class_1 = pd.DataFrame(class_1_data, copy=True)

# If you want to use these two DataFrames for further steps, you can add these lines:
df_class_0.to_csv('class_0_data.csv', index=False)
df_class_1.to_csv('class_1_data.csv', index=False)
class_1_data

# part 3 of question 3 :

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.utils import shuffle
# Assuming you have df_class_0 and df_class_1 from the previous code

# Combine the two classes into a new DataFrame
combined_df = pd.concat([df_class_0, df_class_1], ignore_index=True)
combined_df = shuffle(combined_df)
# Separate features (X) and target (y)
X = combined_df.drop('HeartDiseaseorAttack', axis=1)  # Assuming 'target' is the column you want to predict
y = combined_df['HeartDiseaseorAttack']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Train Logistic Regression model
logistic_model = LogisticRegression(solver = 'sag' , max_iter = 1000 , random_state=83)
logistic_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_logistic = logistic_model.predict(X_test)

# Evaluate the accuracy of the Logistic Regression model
accuracy_logistic = accuracy_score(y_test, y_pred_logistic)
print(f'Logistic Regression Accuracy: {accuracy_logistic:.2f}')

# Train Random Forest model
random_forest_model = RandomForestClassifier(n_estimators=1000, random_state=83)
random_forest_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_rf = random_forest_model.predict(X_test)

# Evaluate the accuracy of the Random Forest model
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f'Random Forest Accuracy: {accuracy_rf:.2f}')

X.shape ,y_pred_rf.shape

# part 3 of question 3 :

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification

# Assuming you have X_train and y_train from the previous code
# If you don't have a 2D dataset, you might need to use PCA or other dimensionality reduction techniques

# Apply PCA to reduce the data to 2D for visualization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_scaled)

# Train Logistic Regression model
logistic_model = LogisticRegression()
logistic_model.fit(X_train_pca, y_train)

# Create a meshgrid to plot the decision boundaries
h = 0.02
x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1
y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1

xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Predict the labels for each point in the meshgrid
Z = logistic_model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot the decision boundaries and regions
plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)

# Plot the examples
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolors='k', marker='o')
plt.title('Logistic Regression Decision Boundaries and Regions')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

# part 3 of question 3 :

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Assuming you have df_class_0 and df_class_1 from the previous code

# Combine the two classes into a new DataFrame
combined_df = pd.concat([df_class_0, df_class_1], ignore_index=True)

# Separate features (X) and target (y)
X = combined_df.drop('HeartDiseaseorAttack', axis=1)  # Assuming 'target' is the column you want to predict
y = combined_df['HeartDiseaseorAttack']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=83)

# Train Support Vector Machine (SVM) model
svm_model = SVC(kernel='linear')
svm_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_svm = svm_model.predict(X_test)

# Evaluate the accuracy of the SVM model
accuracy_svm = accuracy_score(y_test, y_pred_svm)
print(f'SVM Accuracy: {accuracy_svm:.2f}')\


# Train K-Nearest Neighbors (KNN) model
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_knn = knn_model.predict(X_test)

# Evaluate the accuracy of the KNN model
accuracy_knn = accuracy_score(y_test, y_pred_knn)
print(f'KNN Accuracy: {accuracy_knn:.2f}')

# Part 4 :

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import log_loss


model = SGDClassifier(loss='log', random_state=83)
losses = []
epochs = 5000

for _ in range(epochs):

    model.partial_fit(X_train, y_train, [0, 1])
    loss = log_loss(y_train , model.predict_proba(X_train))
    losses.append(loss)

plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss Curve')
plt.show()

# Part 5 :

from sklearn.metrics import confusion_matrix , f1_score
import matplotlib.pyplot as plt
cm = confusion_matrix(y_test , y_pred_logistic)
F1 = f1_score(y_test , y_pred_logistic , average=None)
F1
from sklearn.datasets import make_classification
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_test, y_pred_logistic)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.show()
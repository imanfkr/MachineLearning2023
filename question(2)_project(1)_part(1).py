# -*- coding: utf-8 -*-
"""Question(2)_Project(1)_Part(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19lIk3kKk-I4F76YAevkgAVgKDHmhzeI6
"""

# Part 1 of question two :



! pip install --upgrade --no-cache-dir gdown
! gdown 1EqYX552b90gRE6h19xOKLYZgNkPXaxS_

# Part 1 of question two :

import pandas as pd
import numpy as np
from sklearn.utils import shuffle

file = pd.read_csv (r'/content/data_banknote_authentication.txt')
#file = read_file.to_csv (r'/content/data_banknote_authentication.csv')
headerlist = ['feature1' , 'feature2','feature3','feature4','feature5']
file.to_csv("/content/data_banknote_authentication.csv" ,header = headerlist)
df = pd.read_csv("/content/data_banknote_authentication.csv")
df = shuffle(df)
df

from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression , SGDClassifier
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

X = df[["feature1" ,"feature2" , "feature3" , "feature4"]].values

y = df[["feature5"]].values # target = feature5
X ,y

x_train , x_test , y_train , y_test = train_test_split(X , y , test_size = 0.2)
x_train.shape , x_test.shape , y_train.shape , y_test.shape

def sigmoid(x):
  return 1/(1 + np.exp(-x))

def logistic_regression(x , w):
  y_hat = sigmoid(x @ w)
  return y_hat

def bce(y , y_hat):
  loss = -(np.mean(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))
  return loss

def gradient(x , y  ,y_hat):
  grads = (x.T @(y_hat - y)) / len(y)
  return grads

def gradient_descent(w , eta , grads):
  w -= eta*grads
  return w

def accuracy(y , y_hat):
  acc = np.sum(y==np.round(y_hat)) / len(y)
  return acc

x_train = np.hstack((np.ones((len(x_train) , 1)) , x_train))
x_train.shape

m = 4
w = np.random.randn(m+1 , 1)
print(w.shape)
eta = 0.01
n_epochs = 4000

error_hist = []
for epoch in range(n_epochs):
  y_hat = logistic_regression(x_train , w)

  e = bce(y_train , y_hat)
  error_hist.append(e)
  grads = gradient(x_train , y_train , y_hat)
  w = gradient_descent(w , eta , grads)
  if(epoch + 1) % 100 == 0:
    print(f"Epoch = {epoch} , \t E = {e:.4} \t w={w.T[0]}")

plt.plot(error_hist)

# accuracy :

x_test = np.hstack((np.ones((len(x_test) , 1)), x_test))
x_test.shape

y_hat = logistic_regression(x_test , w)
accuracy(y_test , y_hat)

# normalized data :
maxx = df[['feature1', 'feature2' , 'feature3' , 'feature4']].max()

#print("Maximum value in column 'feature1', 'feature2' , 'feature3' , 'feature4': ")


minn = df[['feature1', 'feature2' , 'feature3' , 'feature4']].min()
#print("Minimum value in column 'feature1', 'feature2' , 'feature3' , 'feature4': ")


for i in range(4):
  df[f'feature{i+1}'] = (df[f'feature{i+1}']-minn[i])/(maxx[i]- minn[i])
  print(df[f'feature{i+1}'])

X = df[["feature1" ,"feature2" , "feature3" , "feature4"]].values

y = df[["feature5"]].values
X , y

x_train , x_test , y_train , y_test = train_test_split(X , y , test_size = 0.2)
x_train.shape , x_test.shape , y_train.shape , y_test.shape

x_train = np.hstack((np.ones((len(x_train) , 1)) , x_train))
x_train.shape

m = 4
w = np.random.randn(m+1 , 1)
print(w.shape)
eta = 0.01
n_epochs = 10000

error_hist = []
for epoch in range(n_epochs):
  y_hat = logistic_regression(x_train , w)

  e = bce(y_train , y_hat)
  error_hist.append(e)
  grads = gradient(x_train , y_train , y_hat)
  w = gradient_descent(w , eta , grads)
  if(epoch + 1) % 100 == 0:
    print(f"Epoch = {epoch} , \t E = {e:.4} \t w={w.T[0]}")

plt.plot(error_hist)

x_test = np.hstack((np.ones((len(x_test) , 1)), x_test))
x_test.shape

y_hat = logistic_regression(x_test , w)
accuracy(y_test , y_hat)

# part 6 :

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# value_count = y.value_counts()
# value_count
new_y = pd.DataFrame(y, columns=['Column_A'])
new_y.value_counts()
new_y.value_counts().plot.pie(autopct = "%.2f")

! pip install -U imbalanced-learn

# undersampling :
from imblearn.under_sampling import RandomUnderSampler

y = pd.DataFrame(y, columns=[''])
rus = RandomUnderSampler(sampling_strategy=1)
x_res_undersampling , y_res_undersampling = rus.fit_resample(X , y)
ax = y_res_undersampling.value_counts().plot.pie(autopct = '%.2f')
_ = ax.set_title("under sampling")

# part 7 :

from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
X = x_res_undersampling
y = y_res_undersampling
x_train , x_test , y_train , y_test = train_test_split(X , y , test_size = 0.2)

model = LogisticRegression()
model.fit(X , y)

y_hat = model.predict(x_test)
model.score(x_test , y_test)
y_test.shape ,
y_hat = y_hat.reshape(244 , 1)
y_test.shape , y_hat.shape

from sklearn.metrics import accuracy_score
score = accuracy_score(y_test, y_hat)
score

# In imbalance mode and using ready libraries :

from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression , SGDClassifier
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

X = df[["feature1" ,"feature2" , "feature3" , "feature4"]].values
y = df[["feature5"]].values
x_train , x_test , y_train , y_test = train_test_split(X , y , test_size = 0.2)
model = LogisticRegression(random_state = 93, solver='sag', max_iter=200)
model.fit(X , y)
y_hat = model.predict(x_test)
model.score(x_test , y_test)